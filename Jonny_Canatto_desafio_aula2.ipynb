{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio - Aula 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nome: Jonny Silva Canatto\n",
    "### RM: 332439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Realizando todos os imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import floresta\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "nltk.download('floresta')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://s3.amazonaws.com/automl-example/produtos.csv\",\n",
    "                 delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) utilizando o df acima carregado, faça:\n",
    "\n",
    "   ### -> Elimine linhas com valores nulos\n",
    "   ### -> Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição\n",
    "   ### -> Quantos Unigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos Bigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos Trigramas existem antes e depois de remover stopwords\n",
    "   ### -> Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)\n",
    "   ### -> Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo duplicados\n",
      "Tamanho total do data frame: 4080\n",
      "Após remoção de duplicados: 3789\n",
      "\n",
      "Removendo nulos\n",
      "Tamanho total com nulos: 3789\n",
      "Tamanho sem nulos: 2646\n"
     ]
    }
   ],
   "source": [
    "#tratamento dos dados\n",
    "\n",
    "#Removendo duplicados\n",
    "print('Removendo duplicados')\n",
    "print('Tamanho total do data frame:', len(df))\n",
    "df = df.drop_duplicates()\n",
    "print('Após remoção de duplicados:', len(df))\n",
    "\n",
    "print('')\n",
    "\n",
    "#removendo nulos\n",
    "print('Removendo nulos')\n",
    "print('Tamanho total com nulos:', len(df))\n",
    "df = df.dropna()\n",
    "print('Tamanho sem nulos:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição\n",
    "df['texto'] = df['nome'] + ' ' + df['descricao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigramas com stop words:  35466\n",
      "Unigramas sem stop words:  35310\n"
     ]
    }
   ],
   "source": [
    "#Quantos Unigramas existem antes e depois de remover stopwords\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df['texto'])\n",
    "text_vect = vect.transform(df['texto'])\n",
    "print('Unigramas com stop words: ', text_vect.shape[1])\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect1.fit(df['texto'])\n",
    "text_vect1 = vect1.transform(df['texto'])\n",
    "print('Unigramas sem stop words: ', text_vect1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas com stop words:  159553\n",
      "Bigramas sem stop words:  145409\n"
     ]
    }
   ],
   "source": [
    "#Quantos Bigramas existem antes e depois de remover stopwords\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df['texto'])\n",
    "text_vect = vect.transform(df['texto'])\n",
    "print('Bigramas com stop words: ', text_vect.shape[1])\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(2,2), stop_words=stops)\n",
    "vect1.fit(df['texto'])\n",
    "text_vect1 = vect1.transform(df['texto'])\n",
    "print('Bigramas sem stop words: ', text_vect1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas com stop words:  228162\n",
      "Trigramas sem stop words:  177869\n"
     ]
    }
   ],
   "source": [
    "#Quantos Trigramas existem antes e depois de remover stopwords\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df['texto'])\n",
    "text_vect = vect.transform(df['texto'])\n",
    "print('Trigramas com stop words: ', text_vect.shape[1])\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(3,3), stop_words=stops)\n",
    "vect1.fit(df['texto'])\n",
    "text_vect1 = vect1.transform(df['texto'])\n",
    "print('Trigramas sem stop words: ', text_vect1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de Verbos na nova coluna: 58842\n",
      "Qtd de Adverbios na nova coluna: 102352\n"
     ]
    }
   ],
   "source": [
    "#Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)\n",
    "# VERB\n",
    "# ADV\n",
    "\n",
    "contador = Counter()\n",
    "verbs = 0\n",
    "advs = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    texto = nlp(row['texto'])\n",
    "    contador = [(token.pos_) for token in texto]\n",
    "    if('VERB' in contador):\n",
    "        verbs += contador.index('VERB')\n",
    "    if('ADV' in contador):\n",
    "        advs += contador.index('ADV')\n",
    "        \n",
    "print('Qtd de Verbos na nova coluna:', verbs)\n",
    "print('Qtd de Adverbios na nova coluna:', advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigramas após aplicar Stemmer sem stop words:  5766\n"
     ]
    }
   ],
   "source": [
    "#Quantos unigramas existem após aplicar Stemmer (utilize rslp)\n",
    "#Unigramas sem stop words:  35310\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "stem = [rslp.stem(x) for x in word_tokenize(df['texto'].to_string())]\n",
    "df_stem = pd.DataFrame({'texto':stem})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df_stem.texto)\n",
    "text_vect = vect.transform(df_stem.texto)\n",
    "print('Unigramas após aplicar Stemmer sem stop words: ', text_vect.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) crie um tagger baseado em expressões regulares:\n",
    "   ### -> crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "   ### -> o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "   ### -> utilize nltk.RegexpTagger(variável) para carregar seu tagger\n",
    "   ### -> apresente uma frase teste para cada tipo de expressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      " \n",
      "[('Estavamos', 'PLUR'), ('andando', 'GRN'), ('na', None), ('rua', None)]\n",
      "[('Varias', 'PLUR'), ('pessoas', 'PLUR'), ('foram', None), ('presas', 'PLUR')]\n",
      "[('Tirei', None), ('dez', 'CARD'), ('na', None), ('materia', None), ('de', None), ('NLP', None)]\n"
     ]
    }
   ],
   "source": [
    "#crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "\n",
    "tupla = (['.*ndo$','GRN'],['[U u]m','CARD'],['[D d]ois','CARD'],['[T t]res','CARD'],['[Q q]uatro','CARD'],['[C c]inco','CARD'],\n",
    "         ['[S s]eis','CARD'],['[S s]ete','CARD'],['[O o]ito','CARD'],['[N n]ove','CARD'],['[D d]ez','CARD'], ['.*s$','PLUR'])\n",
    "\n",
    "print(type(tupla))\n",
    "\n",
    "#o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "tagger = nltk.RegexpTagger(tupla)\n",
    "print(type(tupla))\n",
    "\n",
    "string_gerundio = 'Estavamos andando na rua'\n",
    "string_plural = 'Varias pessoas foram presas'\n",
    "string_cardinal = 'Tirei dez na materia de NLP'\n",
    "\n",
    "print(' ')\n",
    "\n",
    "#Gerundio\n",
    "print(tagger.tag([palavra for palavra in string_gerundio.split(' ')]))\n",
    "\n",
    "#Plural\n",
    "print(tagger.tag([palavra for palavra in string_plural.split(' ')]))\n",
    "\n",
    "#Cardinal\n",
    "print(tagger.tag([palavra for palavra in string_cardinal.split(' ')]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
